<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pkgmetadata SYSTEM "https://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>bratishkaerik@getgoogleoff.me</email>
		<name>Eric Joldasov</name>
	</maintainer>
	<longdescription>
		Part of <cat>llm-gguf</cat> category.
		The Capybara series is the first Nous collection of dataset and models made by fine-tuning
		mostly on data created by Nous in-house.

		We leverage our novel data synthesis technique called Amplify-instruct (Paper coming
		soon), the seed distribution and synthesis method are comprised of a synergistic
		combination of top performing existing data synthesis techniques and distributions
		used for SOTA models such as Airoboros, Evol-Instruct(WizardLM), Orca, Vicuna,
		Know_Logic, Lamini, FLASK and others, all into one lean holistically formed methodology
		for the dataset and model. The seed instructions used for the start of synthesized
		conversations are largely based on highly datasets like Airoboros, Know logic,
		EverythingLM, GPTeacher and even entirely new seed instructions derived from posts on
		the website LessWrong, as well as being supplemented with certain in-house multi-turn
		datasets like Dove(A successor to Puffin).
	</longdescription>
</pkgmetadata>
